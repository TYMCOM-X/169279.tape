                                                              November 13, 1988







                ***** ***** *     ***** ****  ***** ***** *****
                  *   *     *     *     *   * *   *   *   *    
                  *   ****  *     ****  ****  *****   *   **** 
                  *   *     *     *     *   * *   *   *   *    
                  *   ***** ***** ***** *   * *   *   *   *****

                           ***** *****   **** *****
                             *   *      *       *  
                             *   ****    ***    *  
                             *   *          *   *  
                             *   *****  ****    *  

                      ****  ***** ****   ***  ****  *****
                      *   * *     *   * *   * *   *   *  
                      ****  ****  ****  *   * ****    *  
                      *   * *     *     *   * *   *   *  
                      *   * ***** *      ***  *   *   *  


                                ***************   
                               *      NTS      *  
                              *     TELERATE    * 
                             *     ACCEPTANCE    *
                              *       PLAN      * 
                               *               *  
                                ***************   




                          ***************************

                                    TYMNET
                          NATIONAL TECHNICAL SUPPORT
                               November 13, 1988

     ====================================================================
     |   Tymnet,  Inc.'s  proprietary  rights  are  included  in  the   |
     |   information disclosed herein.   The recipient,  by receiving   |
     |   this document,  agrees  that neither this  document nor  the   |
     |   information  disclosed  herein nor any part thereof shall be   |
     |   reproduced or transferred to  other  documents  or  used  or   |
     |   disclosed  to  others  for  manufacturing  or  for any other   |
     |   purpose except  as  specifically authorized  in  writing  by   |
     |   Tymnet, Inc.                                                   |
     ====================================================================



                                                TELERATE Preliminary Procedure*
                                                              November 13, 1988







                                 TABLE OF CONTENTS

    Section                                                                Page

      .1 HARDWARE RESOURCES                                                   1
      .2 TEST PROCEDURES                                                      1
      .3 SOFTWARE PARAMETRICS                                                 3
      .4 THROUGHPUT TESTER                                                    4
      .5 EVALUATION                                                           5
      .6 NETWORK TOPOLOGY                                                     5
      .7 TEST MECHANICS                                                       8
      .8 TYMNET X.25 I/O PROCESSES                                           10
      .9 DIAGNOSTIC COROLLARIES                                              13




































                                                TELERATE Preliminary Procedure*
                                                              November 13, 1988


                              .1  HARDWARE RESOURCES


         The  acceptance plan  configuration requires  specifically two  (2) XL
    Network Switching Nodes, two (2)  HDLC Micro IV X.25 DCE Access  Nodes, two
    (2) Mini X.25  DTE X.25 Simulation Nodes,  one (1) Network  Supervisor Node
    and an additional thirteen  (13) support Network Nodes  for circuit/traffic
    generation/termination.  Internodal time measurements will be acquired from
    an externally  sourced monitor.   The Tekelek Chameleon  32 and  the IDACOM
    PT368 will be utilized to verlfy/qualify the time measurements  reported by
    the Tymnet throghput host reports  as accurate.  The two (2) HDLC  Micro IV
    Access Nodes may  be exchanged with two  Tymnet Pico Access Nodes  to fully
    comply with the Telerate Unit Acceptance and System  Acceptance contractual
    agreements specified in subsection  11.2 paragraph 1 (Pad Function)  at the
    time of the final acceptance testing.  Reference figure 1, A1/A2.






                                .2  TEST PROCEDURES


         The actual testing procedures  will be performed as two  (2) separate,
    discreet processes.  The first process will be of the Pad Functions and the
    second process will be of the Switching Functions.  The Access  Nodes drive
    a total of six (6) each,  physical V.24 I/O ports at 9600 bps  presented to
    the X.25 DTE achieving an  aggregate maximum bandwidth of 7200  CPS in/7200
    CPS  out  with  a   serialization  delay  of  104  u-sec/bit   frame  time.
    Serialization delays to the Network  will equal 52 u-sec/bit frame  time on
    the HDLC Micro IV  Nodes (19.2K bps) and  17.9 u-sec/bit frame time  on the
    Pico Nodes  (56K bps).  The  HDLC Micro Access  Nodes will present  two (2)
    each,  physical  V.24 I/O  ports  at  19.2K bps  to  the  Network Switchers
    achieving an aggregate maximum bandwidth of 4800 CPS in/4800 CPS  out which
    is 37%  greater than  the required 1750  CPS in/1750  CPS out  required for
    baseline load Pad acceptance testing.  The access nodes will distribute the
    aggregate 3500 CPS  out load over the  two network lines across  23 virtual
    circuits.  Virtual circuit  23 will be constructed  to carry the  test data
    for measurement and  will be propagated with  50% of the base  load traffic
    across  the network.   While the  1750 CPS  in from  the X.25  DTE  will be
    distributed  across five  (5) 9600  bps lines  each, only  one (1)  of each
    access node's X.25 lines will be used for the measurement.

         The data  scope's monitor  Y-cable will be  bifurcated to  monitor the
    Receive Data signal on  A1 (Access Node 1)  and Transmit Data signal  on A2
    (Access Node 2)  in the same time  reference.  Any second data  scope's EIA
    monitor cable may be "piggybacked"  on the primary data scope to  provide a
    second reference of the time intervals for on-line verification of all time
    intervals.   Any discrepancy  in time  stamps will  be  immediately obvious
    permitting adjustments to be made to the test "rig" and the  test restarted
    rather than  waiting until  test end to  discover time  discrepancies.  The


    1                                           TELERATE Preliminary Procedure*
                                                              November 13, 1988


    transit time from A1 to A2  plus the X.25 DCE serialization time  (833.3 u-
    sec/byte) will be subtracted from the resultant time stamp and divided by 2
    to determine the individual Access Node intranodal delay.  The transit time
    will be  determined to  be the  sum total  of network  serialization delays
    (416.7 X 2 + 142.9/byte derived from the two 19.2K network lines @  52.1 u-
    sec/bit frame  time and  1 56K network  line @  17.9 u-sec/bit  frame time)
    added to the intranodal  switcher delays.  Individual switcher  delays will
    be determined by first measuring an individual packet delay from A1 through
    A2 as it transits  both switchers S1 and S2  and then removing S2  from the
    network path routing S1 directly to A2.  The same packet will be sent again
    from  A1 through  A2  and the  resultant  delay subtracted  from  the delay
    through  both  switchers.   The  difference  offset  by  the  serialization
    deviations of the 56K bps line  removed from the path with the  S2 switcher
    will be  equal to the  internodal delay  of S2.  To  verify that  the delay
    introduced by S2 is typical and not extraordinary, S1 will be  removed from
    the circuit  path and S2  connected directly to  A1.  The average  delay of
    (S1+S2)/2 will then be used as  a K constant for the remainder of  the X.25
    Access test processes.

         The Switching Functions acceptance testing will target nodes S1 and S2
    specifically.  These Mini-XL's will  be evaluated with the  same techniques
    utilized to  test the  access nodes.  The  Access Nodes  X.25 slots  may be
    reconfigured  with  the throughput  host  to used  for  circuit performance
    measurements.  The base loading for the switching nodes will be established
    at 12,150K  CPS in/ 12,150K  CPS out per  switcher. The test  circuit under
    analysis  will transit  the switchers  with the  load traffic  providing an
    accurate representation of the real switching intranodal delays.   The test
    circuit  may  be  constructed  via the  existing  X.25  Access  Nodes whose
    emperically  determined  delays will  be  subtracted from  the  end  to end
    measurements.  All logical test packets will contain 16d bytes of data each
    which translate to physical 24d  byte Tymnet T201 protocol packets  for the
    circuit under measurement.

         A1 and A2 Access Nodes  average intranodal delays will be equal  to or
    less than 35.0 milliseconds from the last bit of the packet in to the first
    bit of the packet out 99.5% of the time.  S1 and S2 Switching Nodes average
    intranodal delays will be equal to or less than 12.0 milliseconds  99.5% of
    the time.  As contractually specified 1000 passes of each test will be run.
    Baseload traffic will be constructed across 22 virtual circuits through the
    Access Nodes.   Eight (8) circuits  of 51 Byte/X.25  packet plus  seven (7)
    circuits of  81 Byte/X.25 packet  and seven (7)  circuits of  111 Byte/X.25
    packet for a total of 1752 CPS in and 1752 CPS out.

         Access nodes  will be  coded for  a maximum  of 700  virtual circuits.
    Intranodal peak delays will  be equal to or  less than three (3)  times the
    calculated mean  time delays of  35.0 millisecond per  Access Node  or 12.0
    millisecond per Switching Node.

         Declaring L0  to be  the calculated X.25  serialization delay  at 9600
    bps, L1, L3 to be the calculated T201 serialization delay at 19.2K  bps and
    L2 to be the calculated T201  serialization delay at 56K bps and TDx  to be
    the total network delays the following formulas presented by Ron Vivier may
    be used to derive individual nodal delays:

    2                                           TELERATE Preliminary Procedure*
                                                              November 13, 1988



    Total network delay (2 switchers)

            A1 + L1 + S1 + L2 + S2 + L3 + A2 - L0 = TD1

    Total network delay (1 switcher)

            A1 + L1 + S1 + L3 + A2 - L0 = TD2

    Intraswitcher nodal delay

            (TD1 - TD2) - L2 = SD1

    Intraaccess nodal delay

            [(TD2 - SD1) - (L1 + L3)] / 2 = AD1


         Under  actual  load  test  analysis  intranodal  diagnostic  and event
    tracing  will  be  constrained  to  reduce  unnecessary   machine  resource
    utilization so that all Acceptance test data accumulated will be guaranteed
    to  be  not  only  accurate, but  acquired  with  non-Tymnet  equipment and
    monitors as much as is  possible.  Tymnet's PROBE program which  runs under
    each network Supervisor  will be used to  monitor the status of  the hosts,
    nodes and lines in the Telerate test network.  PROBE will also  be utilized
    to alter/control  the exact routing  of the base  load traffic  circuits as
    well as  the test  measurement circuits  to maintain  balanced, symmetrical
    load conditions.

         Intraswitcher  nodal delay,  intraaccess nodal  delay and  end  to end
    circuit delay data will be graphically represented as plots of delay values
    vs  CPS  values  and  mathematically  presented  as  functions   which  are
    differentiable at any given interval of the test period.






                             .3  SOFTWARE PARAMETRICS


         Each  node  is  configured by  software  declarations  in  the machine
    Tymfile.  The Tymfile contains  the variables which determine  the assembly
    time values.  The most common categories of the Tymfile  statements include
    constraints  for  hardware  configuration,  ISIS  configuration,  Node Code
    configuration and network configuration.  Additionally, ISIS  slot Tymfiles
    further determine machine resources assigned to specific  applications such
    as the X.25 DCE slot code and the Throughput traffic generator.   Copies of
    each specific  test Tymfile  will be  presented with  the test  results for
    review.   Comments  explaining  briefly  the  functions  of   each  Tymfile
    statement are included with the Tymfiles.


    3                                           TELERATE Preliminary Procedure*
                                                              November 13, 1988


         Command files use Tymnet's NAD commands to control the assembly of the
    nodal codes.  The versions of software used to assemble the  object machine
    code are defined in these  command files.  Copies of each  specific command
    file will also be presented with the test results for review.  The Telerate
    Tymfiles, command files object code  and source codes will be  written onto
    Tymnet System 37 upon  which Telerate already has a  working user-directory
    (TELCODE:37).






                               .4  THROUGHPUT TESTER


         The Throughput  Testing Host  (AKA TPUT) is  a tool  used to  test the
    performance  of  a network  or  specific  functions of  a  network.   It is
    constructed to allow the user to evaluate load capacities  and efficiencies
    of a network.  The TPUT Host running in the Telerate test network  has been
    enhanced  to permit  the  generation of  up to  1024  simultaneous circuits
    across the network.   The user has the  option of specifying the  number of
    circuits  built, the  frequency and  quantity of  data transmitted  and the
    frequency and format of the statistical reports generated by the TPUT Host.
    This is the vehicle which will be utilized to establish the network traffic
    loads and  specific test circuits  of fixed packet  size.  Two  (2) network
    host numbers are  associated with each TPUT  generator.  The first  will be
    logged onto from  a terminal which will  be assigned the  highest available
    slot port number and will be considered the controlling or "boss" terminal.
    It is this boss terminal which will command the throughput  load processes.
    These  processes may  be  invoked either  manually  or from  a  script file
    process.  As we will  be taking 1000 samples  of each test in  progress and
    not necessarily starting  and restarting the  TPUT test excessively  it may
    well suffice  to generate the  loading manually and  direct some  effort to
    programming the IDACOM and the Tekelek to trigger and  sample automatically
    for the sampling tasks.  The histogram of the TPUT host may be  utilized to
    calculate the mean and peak delays.

         The 70%  CPS capacity loading  of the Access  and the  Switching nodes
    will  be achieved   via the  TPUT Host(s).   The base  traffic load  on the
    Switching  Nodes  will  result  from  circuits  originating  in  TPUT slots
    resident in support nodes adjacent  to the target nodes S1 and  S2, through
    the switchers terminating in other  nodes adjacent to S1 and S2  with their
    own resident TPUT Host slots.  The circuits are to be constructed from both
    directions through the switchers to provide a realistic, bidirectional load
    representative of a live Telerate scenario.

         The X.25 base load and test circuits will be constructed in a multiple
    step procedure.  A circuit will originate from a TPUT Host to the  X.25 DCE
    Host.  The X.25 DCE interface will "home" the circuit to the DTE across the
    9600 bps access line.  The DTE  will prompt the non-packet mode host  for a
    called address field to be used in the call request packet sent  across the


    4                                           TELERATE Preliminary Procedure*
                                                              November 13, 1988


    X.25 link.  As noted in previous documents, the destination address  may be
    in any  of several  forms.  In  the Telerate  test the  response will  be a
    secondary logon to a TPUT Host resident in the X.25 DTE simulation machine.
    As in the switching scenario, the circuit origination/termination  may also
    be bidirectional.  For the specific test circuit under analysis the circuit
    will  be  constructed  and  evaluated unidirectionally  from  A1  to  A2 as
    reflected by the bifurcated cable configuration.






                                  .5  EVALUATION


         The measured test results  from the 1000 sample/test will  be collated
    and checked for consistency.  However  we choose to present the  test data,
    the ratio of  the sum total  of Access Nodal delays  to the number  of test
    samples  at a  fixed baseline  of 3500  CPS in/out  will equal  the average
    intraaccess  nodal  delay  which  must  be  equal  to  or  less  than  35.0
    milliseconds.  The ratio of the sum total of Switching Nodal delays  to the
    number of test samples at a  fixed baseline of 24.3K CPS in/out  will equal
    the average intraswitcher nodal delay  which must be equal to or  less than
    12 milliseconds.






                               .6  NETWORK TOPOLOGY


         See Figure 1 and attached Node to Host translation table.  Install the
    IDACOM/Tekelek data scopes on Access Nodes 2132/2133 physical ports  2.  We
    will install  the bifurcaable  to monitor Received  Data on  Node 2132
    port 2 input and Transmitted Data on Node 2133 port 2 output.
















    5                                           TELERATE Preliminary Procedure*
                                                              November 13, 1988


                               NTSNET TELERATE TEST BED
     
                NODE  NODE #   KHOST      TPUT  HOSTS      X.25  HOSTS
     
                S1    2130     608
     
                S2    2131     609
     
                A1    2132     610                         2710, 2711, 2712
                                                           2713, 2714, 2715
                                                           2719 XOM
     
                A2    2133     611                         2720, 2721, 2722
                                                           2723, 2724, 2725
                                                           2729 XOM
     
                N1    2134     612        2000, 2001       2730, 2731, 2732
                                                           2733, 2734, 2735
                                                           2739 XOM
     
                N2    2135     613        2002, 2003       2724, 2741, 2742
                                                           2749 XOM
     
                N3    2136     614        2004, 2005       2750, 2751, 2752
                                                           2759 XOM
     
                N4    2137     615        2006, 2007       2760, 2761, 2762
                                                           2769 XOM
                N5    2032     546        2770,2771
     
                N6    2034     548        2018, 2019
                                          2020, 2021
     
                N7    2030     544        1974, 1975
                                          2262, 2263
                N8    2031     545
     
                N9    2142     618        2038, 2039
                                          2040, 2041
     
                N10   2143     619        2042, 2043
                                          2044, 2045
     
                N11   2035     549        2000, 2001
     
                N12   2140     616        2034, 2035
                                          2036, 2037
     
                N13   2144     620        2046, 2047
                                          2048, 2049
     
                N14   2145     621        2008, 2009
                                          2700, 2701

    6                                           TELERATE Preliminary Procedure*
                                                              November 13, 1988


     
                N15   2033     547        2014, 2015
                                          2016, 2017







                                .7  TEST MECHANICS











































    7                                           TELERATE Preliminary Procedure*
                                                              November 13, 1988


     I.  Quality Ceritification of Test Environment

         Purpose & Objective of this section I is to establish
         confidence that all equipment is properly configured
         and performing nominally:

         A.  Data Scope Calibration

             1.  Check Service/Calibration Dates of Scopes
                 Record Dates/Serialization

             2.  Pass test packet from Scope to Scope, confirm time
                 intervals are equal relative to each other
                 Record baseline interval ratios/averages

         B.  Network Event Monitors

             NTSNET Circuit Multiplexer Host 0010 will be utilized as a
             Primary Network LOGON point.  All subsequent circuits will
             be constructed and "pathed" to via MUX 10, Username: NTS.
                 
             1.  LOGON to (TELERATE:37) set Aux Circuit license and open
                 RECORD file to log all subsequent data into.  Filespec =
                 PROBE.LOG.  Execute GO (SPL)TAUX and LOGON to NTSNET
                 gateway Host 267.  Upon transit of gateway, LOGON to
                 PROBE process.  (Usernames/passwords will be provided
                 to STI and TELERATE consultants at actual test time,
                 not for publication)

            2.   PROBE status of network
                 Execute/record the following:

                 a:AN (All Nodes, Node types, Node Code versions, Host
                      numbers, crash counts and upstream Node numbers)

                 b:LSHUT (List all pairs of Nodes with Shut links, 
                         updated only once/minute)

                 c:NODE XXXX (XXXX = variable for each Node in Telerate
                             Test network.  Execute for each Node on 
                             Figure 1 Topological Reference Sheet)

            *    At this time leave the PROBE log file open and prepare
                 for another network LOGON.  Do not execute the SHUT
                 commands prior to performing the individual Nodal
                 diagnostics.

    *            d:NSHUT YYYY (YYYY = variable for Nodes 2130, 2131,
                              2132, 2133, 2134, 2135, 2136, 2137)

    *            e:LSHUT (Wait 1 minute and verify shuts of target Nodes)



    8                                           TELERATE Preliminary Procedure*
                                                              November 13, 1988


             C.  Nodal Event Monitors

                 3. LOGON to (TELRATE:37), open a second RECORD file,
                    filespec = DDT.LOG.  Set the AC license and LOGON
                    to NTSNET gateway Host again.  LOGON to DDT processes
                    in primary Nodes 2130, 2131, 2132, 2133, 2134, 2135,
                    2136, 2137  and perform the following commands to
                    establish baseline evaluation off primarily the 
                    Switching Nodes, secondarily the Access Nodes and
                    tertiarily the DTE Simulation/Support Nodes:

                    a:?STAT (Slot 00 only)

                    b>ND

                    c>NS

                    d>KSS

                    e>SS

                    f>BS

                    g>PQ

                    h>HS

                    i>CL20

                    j>[CNTL]C, ?CPU, ?LU, ^FF, ?VERN

    *               At this step LOGOFF from Nodes 2130, 213 and
                    proceed to examine the Access Nodes and DTE
                    simulators executing additional commands.  

                 4. Access Node and Support Node DDT commands:

                    a:^1, ?CPU, ?HOST, ?LU, ?VERN

                    b:?STAT, SIO, [CNTL]C

    *               Determination of fundamental machine conditions is
                    complete.  Leaving the existing DDT circuits pathed
                    to Nodes 2132, 2133, 2135 and 2137 Slot 01 return to
                    the original PROBE Aux circuit from (TELRATE:37) and
                    execute the commands specified in section B2d (NSHUT)
                    to control all subsequent circuit routing throughout
                    the testing.
            
             D. Establish that the methodology prescribed in the Acceptance
                Unit/System test plan performs as advertised, and establish
                the baseline end to end traffic delay time measurements.


    9                                           TELERATE Preliminary Procedure*
                                                              November 13, 1988


                 1. LOGON to TPUT Hosts 2000, 2002, 2004, 2006 and
                    generate discreet circuits with 20 byte/packet
                    data transfers

                   a:Return to PROBE and remove link shuts from Node pairs
                     2132-2133, 2134-2135, 2135-2136, 2136-2137, 2034-2134,
                     2034-2135 enabling symmetrical end to end circuit paths
                     from TPUT Host through Access Nodes and parallel TPUT
                     Hosts with the RSHUT command.

                   b:From MUX 10 path to TPUT Hosts 2000, 2006

                   c:Make 1 circuit, M1

                   d:LOGON to DTE simulator, NTS:2750;password[CR][CR]
                     DTE will "home" circuit to A2 DCE Host 2720
            
                   e:LOGON from A2 DCE to A1 DCE, NTS:2710;password[CR]
                     DCE will be "homed" circuit to DTE Host 2730 

                   f:LOGON from DTE Host 2730 to TPUT Slave Host 2007,
                     NTS:2007;password[CR][^Z]

                   g:Set a BURST value of 16 Bytes to send to ISIS, B16[CR]

                   h:Set a CPS rate of 16 CPS, C16[CR]

                   i:Output the data bursts, N[CR]








                           .8  TYMNET X.25 I/O PROCESSES


         It is appropriate at this moment to provide a brief explanation of the
    theory of operation driving the  Tymnet X.25 which will be used  to observe
    the  data flow  through the  Access Nodes  I/O channels.   In  the Telerate
    application each Tymnet DCE Host will service one (1) LAN over a  V.24 9600
    bps I/O port.  As we expect to normally service three (3) LANs and  six (6)
    LANs under full backup duty  cycles, the Acceptance X.25 test  presents six
    (6) Hosts/Ports  per slot  interface.  Each port  translates to  a specific
    logical unit which has  a unique serial I/O  area.  The start and  end slot
    addresses (virtual)  are located  at XCOM  labels SIOTn  and SIOXn.   Let n
    equal the slot logical  unit number 0 - 5  for the six LUs assigned  to the
    Telerate interface.

         At the start of the SIO area for each LU is the SIO status  area.  The


    10                                          TELERATE Preliminary Procedure*
                                                              November 13, 1988


    contents of  the Zilog  SIO device  Read Registers  R0,R1 are  written here
    along with  miscellaneous error  counts (CRC, ABORT,  IDLE counts)  and any
    non-zero  SIO interrupts  with  an associated  FASTC time  stamp.   The SIO
    command issued as per instructions  under the DDT commands will  be reading
    this field.  Non-zero return codes here would be: 2 - CCW  (Channel Command
    Word) error, 6 - Receive overrun/Transmit underrun or 8 - I/O Command Error
    (at  which time  we would  probably stop  and replace  the  IOC processor).
    Following the status area are the initialization, transmit, receive channel
    programs and their transmit, receive buffers.

         INCWn, the slot's X.25 input channel command program, sets up the size
    of the input buffer and the maximum record size allowable determined in the
    X.25 slot Tymfile.  The input channel routine is executed either  after the
    slot start execution  time, LUn line  initialization and/or after  the non-
    zero return status code 6 is read (receive underrun).  We will  monitor the
    receive buffer which follows the  INCWn area for the 20 byte  packets being
    sent  from  the DTE  simulator.   The  address for  the  receive  buffer is
    associated with the label RBUFn.  Specifying the NAD command %P in the slot
    command file at assembly time will put the state information and all of the
    symbols we  will use  for debugging and  verifying the  Acceptance process.
    The first halfword  of RBUFn should be  FFFFx indicating a record  has been
    read in.  8 bytes later is the byte count of the data record  read followed
    by the input data and another halfword FFFFx which marks the current end of
    the last record  received.  The receive buffer  is a circular  buffer which
    will wrap  when the  received record can  not be  aligned in  the remaining
    RBUFn area.

         The SIO Tymnet  X.25 output structure  is divided into  three separate
    output data fields.  A transmit rotor (control) list and two  separate data
    areas,  an I-Frame  and a  non-I-Frame field.   We are  most  interested in
    observing the 20 byte CCWs (packets) in the I-Frame field.  The  rotor list
    of commands field length is  equal to the max packet window  size specified
    in the slot Tymfile for the link.  Each entry consists of an output command
    followed by a jump to the next output command.  The last entry in the field
    contains  a jump  to the  first  entry.  With  each new  entry,  the output
    command, address  of its output  buffer are put  on the list.   If chaining
    commands the  jump is placed  in the  entry being chained  to and  the next
    entry  gets its  output command  and address.   If chaining  the completion
    status is 100x, if complete the  status is 200x.  By reading the  fields at
    the addresses  associated with the  labels TCCLn (transmit  channel control
    list), OSECn (output sector) and  TCCWn (transmit CCW) we will  monitor the
    size and state of the output X.25 records.












    11                                          TELERATE Preliminary Procedure*
                                                              November 13, 1988



            2.  Return to DDT circuits and verify origination DTE is
                transmitting 20 Byte packets to A1 DCE.
                Access Nodes 2132,2133 monitor address fields:

                ICCW0=21C50x  RBUF0=221E0x  TCCW0=21D80x  SIOT0=21C00x
                ICCW1=22750x  RBUF1=22CE0x  TCCW1=22880x  SIOT1=22700x
                ICCW2=23250x  RBUF2=237E0x  TCCW2=23380x  SIOT2=23200x
                ICCW3=23D50x  RBUF3=242E0x  TCCW3=23E80x  SIOT3=23D00x
                ICCW4=24850x  RBUF4=24DE0x  TCCW4=24980x  SIOT4=24800x
                ICCW5=25350x  RBUF5=258E0x  TCCW5=25480x  SIOT5=25300x

                a:Read the address fields specified in the table to
                  verify the actual data records being received and
                  transmitted on the port.  

                b:Open a receive file on the data scopes and record the
                  transitting data to disk with time interval stamps for
                  correlation being the internal Nodal data descriptors
                  and the independent external descriptors created.

                c:LOGOFF from all internal DDT processes on all nodes at
                  this time, reset the data scopes acquisition sampling
                  and record 1000 samples of the single channel end to end
                  transfer.

                d:MUX to TPUT Host 2002 and create an additional 14
                  circuits through the DTE Hosts 2751, 2752 which again
                  home to the Access DCE Hosts at which time the circuits
                  will be directed to TPUT Slave Host 2005.

                e:Set the Burst to 81 and 111 as on the measured circuit
                  and set for continuous output.  Set CPS rates of 81 and
                  111 per circuit to achieve the background load across the
                  Access Nodes Network link.   Return to PROBE,
                  Shut the 2132-2133 link, RShut the links 2130-2132 and
                  2131-2133, return to Host 2004 and construct the additonal
                  8 circuits through Hosts 2760-2723-2740-2713, 2761-2724-
                  2741-2714 and 2762-2725-2742-2715 all destined to TPUT
                  Host 2003.  Set a Burst of 51 and CPS of 51 per circuit.
                  Open new receive files and sample 1000 passes.


         We now have a total end to end delay from A1 to A2 with a load of 1750
    CPS in + 1750 CPS out each Access Node.  Halt the testing at this  time and
    kill the test measurement circuit and rebuild it over the network path from
    A2-S2-S1-A1 to measure the added Switcher delays.

         Repeat exactly every previous  test up to this point.   The difference
    between  the  first  time  intervals and  the  second  time  intervals will
    translate to the delays of Switching Nodes S1 and S2 under a single circuit
    16 byte /packet load with up to  a 3500 CPS load.  Open a new  receive file
    to save 1000 samples to data scope disks.

    12                                          TELERATE Preliminary Procedure*
                                                              November 13, 1988


         At this time remove S1  from the circuit path and directly  connect S2
    to A1,  and again  repeat every previous  test for  1000 passes.   Save the
    samples  to  disks.   We  now  can  calculate  the  delay  of  Switcher S1.
    [(DS1+DS2)/2 - the 56K bps serialization delay] will establish  the average
    XL Switcher  Nodal delays.   We can at  this time  also calculate  then the
    Access Nodal delays from the end to end time stamps less  serialization and
    Switching Nodal delays.

         At this time reestablish the "normal" S1 to S2 56 KB link  and connect
    A1 and A2 as per Figure 1.  Return to the PROBE circuit and remove the link
    shuts from 2132-2133.  The test  bed has been modified to  provide switcher
    node 2131 with four  (4) network neighbors all  on 56 KB, V.35  lines.  The
    baseload traffic will be constructed  across 2131 from these four  nodes at
    the rate  of 12,150  CPS.  The measured  circuit will  still be  pathed via
    nodes  2132,  2133.   The  Switching  Node  performance  tests  can  be now
    completed.  From PROBE execute the LSHUT to review the remaining shut links
    and remove all shuts between  the Switching Nodes and their  neighbors with
    the   resident   TPUT   load   Hosts   (2030,2031,2142,2143,2035,2140,2144,
    2145,2003,2034).  The traffic  load will be a  bidirectional, symmetrically
    balanced load.  Each of the Switching neighbor nodes contains two  (2) TPUT
    load Hosts.






                            .9  DIAGNOSTIC COROLLARIES


         At acceptance  test end  point an  attempt will  be made  to correlate
    externally derived measurements from the data scopes/ monitors and Tymnet's
    debugger event monitor reports.  Most interesting to correlate will  be the
    ?Host and ?CPU commands under  the DDT processes. The Slot 00  extended DDT
    or XRAY commands of interest are certain Node Status, Link and Line Status,
    Channel  Status,  Buffer/Bufferlet commands,  Crypto  Message  commands and
    Delay  Measurement  commands.  As  the  events reported  by  the  PROBE are
    subjugate to the events  passed to it via  the individual nodes it  is most
    appropriate   to  exclude   the   PROBE  messages   from   the  measurement
    correlations.

         The ISIS environment is comprised  of three (3) major tasks  which are
    the Kernel, Dispatcher and Slots.  Each task has its own section of machine
    resources  (ie  memory) allocated  to  it and  receives  periodic  CPU time
    "slices".  Utilizing the  Tymnet debugging/monitor tools for  evaluation of
    machine performance and subsequent correlation of these with any externally
    derived performance measurements requires at the least, a common definition
    of certain Tymnet terminology, Node processes and timing mechanisms.

         The  Kernel schedules  CPU time  slices for  all job  Slots  which are
    divided  into  three  (3)  main  processes  defined  in  the  Engine  to be
    Foreground (FG), Background (BG)  and Dynamic Debbuging Tools  (DDT) tasks.


    13                                          TELERATE Preliminary Procedure*
                                                              November 13, 1988


    Time for all processes is  allocated in "TICKS".  Each "TICK"  equals 1/600
    second or 1.6667 millisecond which  will be the basis for  the intramachine
    delay/performance measurements.   The Kernel does  not queue or  buffer any
    data!   It only  runs the  processes and  moves data  to/from  Slots.  Slot
    interfaces request actual data  transfers to/from the software  I/O drivers
    through the Kernel.

         The Foreground  processes communicate  with interrupt-level  I/O tasks
    and require frequent, short allocations  of CPU time.  The FG job  queue is
    checked before  the Kernel  schedules a process.   Active FG  processes are
    scheduled to execute every 25 milliseconds or in other words, 15 TICKS with
    a minimum of 2 TICKS plus 1 TICK per every 3 logical units assigned  to the
    Slot.  The  default FGFREQ  value of 15  TICKS may  be modified  at machine
    assembly time as specified  in the machine Tymfile.  Setting  the frequency
    to  7  or  8  would  direct  the  Kernel  to  schedule  the   FG  processes
    approximately twice as often which would of course, take CPU time away from
    the housekeeping Background processes where the real work is  taking place.
    One  of  the  "real" jobs  referenced  here  would be  the  task  of packet
    assembly/disassembly.  These BG jobs are themselves divided into timeslices
    of 50 milliseconds which are interruptable as the FG requires.  It  will be
    attempted to modify the FG/BG time slices, measure the  machine performance
    statistics  and  create  characteristic  functions  which  reflect  machine
    throughput  capacities  and  delays.   The  statistics  are  to  be plotted
    graphically,  the  functions  will  be  derived  and  given  that  they are
    continous  functions  and  differentiable  the  first  derivative   of  the
    functions  at   any  given  interval   will  be  calculated   to  represent
    mathematically not only if the throughput and/or delay is increasing at any
    selected interval, but also how fast it is changing.

         The Dispatcher processes move  data between job Slots and  the Kernel.
    The Dispatcher is  run immediately following any  BG process.  If  there is
    data to  be moved to/from  any Slot, the  Dispatcher routes the  data.  The
    Dispatcher uses its own buffer mechanism, a circular ring for I/O from Slot
    to Slot with its own backpressure mechanism to prevent loss of data  in the
    ring.

         Measurement of data  delays through the individual  nodes incorporates
    the measurement of  the data as it  moves to/from the external  I/O logical
    units  during the  FG intervals,  through the  Slot interface  where  it is
    manipulated for presentation to/from the Dispatcher during the BG processes
    and then to/from  the Node Code Slot  which again must manipulate  the data
    during the BG periods  to present the data  during the FG intervals  to its
    own logical  units.  The  Node Code  itself is  responsible for  the Tymnet
    internodal  protocol,  Record  (packet) Tear  Down  (RTD),  Record (packet)
    Making (RMAKE),  Permuter table modification  and data flow  control across
    the network.  Please note that  RTD and RMAKE operations are  FROZEN during
    periods of link bubble and link shrink as the Permuter Table space  for the
    link is  being expanded or  reduced.  In the  TELERATE test  environment we
    will  direct  that  all  multiple virtual  circuits  assigned  to  links be
    constructed   and  stable   prior  to   actual   performance  measurements.
    Theoretically, a link on a Tymnet Node can bubble up to 8192 channels or 32
    pages of 256 channels (1/2 page per bubble).  Also note that the  Node Code


    14                                          TELERATE Preliminary Procedure*
                                                              November 13, 1988


    recognizes throughput classes of 64, 160, 512 and a maximum of 2048 CPS per
    circuit so  we will endeavor  to avoid exceeding  this constraint  to avoid
    excessive  circuit backpressure  variance.  The  XRAY Crypto  message event
    logger will provide the status of the link bubble/shrink events.

         The XRAY  Channel Status commands  AC/TC/TD/TA should prove  useful to
    correlate  intranodal  channel  trace  and  time  stamps  (FASTC)  with the
    external data  scope trace and  time stamps.  When  the trace  channel (TC)
    command is enabled for an active channel, the routines RTEAR and RMAKE will
    update the  trace fields with  the I/O data  and the associated  FASTC time
    when  that  data  record  was  made or  torn.   The Node  Code  (XRAY) time
    intervals reported are expressed  in several different ways.  FASTC  is the
    1/600  second  TICK,  SLOWC is  exactly  1  second and  Node  Code  time is
    expressed as GMT.   As correlation will be  made between these  time stamps
    and the data  scope/monitor time stamps  please consider the  principles of
    the RTD and RMAKE operations.

         RTD scans  all active  links for records  (packets) arriving  from the
    network neighbors.  When detected, RTD  will tear it down into  its logical
    records, check the permuter table for an assigned buffer number to send the
    data from the records to and  write it into a buffer.  The  channel number,
    which we will trace, is used as an index into the permuter table to get the
    buffer number.  Once written into that buffer the data is then routed along
    the proper virtual  circuit (VC).  Each  VC  has a buffer pair  assigned to
    it.  The buffer pair is used to  permit a type of full duplex data  flow to
    exist.  The buffers contain pointers  to 16 byte long bufferlets  where the
    actual data is  located.  Each bufferlet contains  14 bytes of data  plus 2
    bytes of pointer to the  next bufferlet linking all bufferlets in  a buffer
    together.  Our receive trace will time stamp the moment when RTD performs a
    write  into  the bufferlet.   RMAKE  scans  an array  of  buffer  flags for
    indicators  of data  present in  output buffers  to packetize.   RMAKE will
    construct a  logical record  record of up  to 121  bytes (->128  bytes with
    overhead).  Typically  RMAKE can  assemble up  to 7  unacknowledged packets
    ahead (windowsize 0-7).  Our transmit trace will time stamp the moment when
    RMAKE assembles the logical record to send out.

         The most  readily obvious and  quickly obtainable corollaries  will be
    those made by simply reproducing the exact tests executed during the Access
    Pad functions and Switching functions acceptance testing with  the external
    data scope/monitors in-line.  Whereas we constrain the Acceptance  tests to
    permit no DDT or XRAY  circuits into the target nodes, these  tests dictate
    that the same machines  now have debugger circuits active.   The difference
    of the immediate results less the Acceptance test results will  provide yet
    another base  offset or constant  K which is  to be  appropriately factored
    into machine performance  measurements "under the influence"  of diagnostic
    circuits.

         XRAY Node Status commands ND and NS will be used for some correlation.
    The ND command displays status of the nodes configration and network links.
    Of interest  here are the  PORTS, PASSTHRUS, ALNK,  LINK WSIZ,  LINE SPEED,
    LINK  CRQS  and LINE  ERRATE  fields.   The NS  command  displays  1 second
    averages and peak values  of selected operating paramenters used  by Tymnet


    15                                          TELERATE Preliminary Procedure*
                                                              November 13, 1988


    operations in evaluating node load.   Of major interest here are  the EXCT,
    EXLW, EDHW, SYHW, MPRT,  RMK XCT, RMK NMK, PACKETS,  TOTAL LOGICAL-PHYSICAL
    I/O CPS RATES, CODE+H-WARE  DELAY and PROCESS fields.  Please  notice these
    are in fact documented XRAY commands/fields and should be referenced in the
    XRAY user manual for greater details of specific field descriptors.  The KD
    and  KS  commands provide  similar  statistics but  more  specific  to each
    neighbor link.  These will be used for details of specific  channel numbers
    to  trace, bandwidth  utilization reports,  packet overhead  and  node code
    queuing delays.

         The  BS  command is  primarily  used to  estimate  the  optimum memory
    allocations  for the  node  code but  as implemented  provides  yet another
    barometer of the machine performance.  The total amount of memory allocated
    for buffer  storage for node  code is  displayed along with  a snap  of the
    amount of  memory currently in  use for  buffer storage as  well as  a peak
    history  of the  most  memory ever  used  for buffer  storage.   As machine
    loading  increases the  current buffer  storage capacity  can  be observed.
    Should the  machine become  loaded to  the point  where the  current buffer
    space in use increases with every subsequent snap (update every  2 seconds)
    we can  quickly calculate the  rate of change,  the amount of  buffer space
    remaining and  project how  long the node  can function  until a  node code
    crash.  The node  code will crash  if all buffers are  in use and  the node
    code needs to buffer up any  more data.  This may prove useful  to TELERATE
    to create  a worst case  overload scenario to  provide an early  warning of
    node overload and impending failure.

         The  CL command  fields will  be observed  for the  link bubble/shrink
    events,  line  saturation/overload conditions,  retransmissions  and Access
    Host  Status reports  (HSRs).   Correlation may  be made  between  the line
    status  reports  in  the  crypto  log  relative  to  specific   node  delay
    measurements.

         The Delay measurement  commands allow the  XRAY user to  build special
    test  circuits between  nodes and  monitor round-trip  delay on  them.  The
    delays reported with the DD command are the sum of the link  and processing
    delays along the  circuit path dictating that  the network link  delay will
    always be less than the  reported delay measurement.  The frequency  of the
    test  packets  and the  size  of the  packets  are under  user  control.  A
    correlation between the test data derived from the Acceptance testing which
    plots CPS  vs DELAY  can be readily  made with  the XRAY  delay measurement
    reports given an  equivalent load is constructed  via the XRAY  facility as
    was via the TPUT hosts.

    End of NTS corollary.;  }










    16                                          TELERATE Preliminary Procedure*
    (@2Ô